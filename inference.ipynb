{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q xformers\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install -q einops\n",
    "!pip install -q wandb\n",
    "!pip install -q scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/falcon-7b-sql/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "dataset_id = 'spider'\n",
    "spider_schema = '/workspace/falcon-7b-sql/data/tables.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_handler import get_dataset\n",
    "dataset = get_dataset(dataset_id, spider_schema, use_fields=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.inference_dataset import InferenceDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_batch_size = 1\n",
    "\n",
    "inference_ds = InferenceDataset(dataset['validation'])\n",
    "inference_dataloader = DataLoader(inference_ds, batch_size=eval_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from model import get_pretrained_model_and_tokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_id = 'maidacundo/falcon_qlora_sql'\n",
    "\n",
    "model, tokenizer = get_pretrained_model_and_tokenizer(model_id, bnb_config, lora_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['input_text'].split('<|sql|>')[-1])\n",
    "\n",
    "tokenized_ds = dataset['validation'].map(\n",
    "    preprocess_function,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    ")\n",
    "\n",
    "input_ids_lengths = []\n",
    "\n",
    "for prompt in tokenized_ds:\n",
    "  input_ids_lengths.append(len(prompt['input_ids']))\n",
    "\n",
    "\n",
    "percentiles = np.percentile(input_ids_lengths, range(0, 101, 10))\n",
    "\n",
    "plt.plot(range(0, 101, 10), percentiles)\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Input IDs Length\")\n",
    "plt.title(\"Percentile Distribution of Length\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference_utils import get_pipeline, generate_pipeline\n",
    "pipeline = get_pipeline(model, tokenizer)\n",
    "results = generate_pipeline(pipeline, inference_dataloader, tokenizer.eos_token_id, tokenizer.pad_token_id, max_new_tokens=160) # max new tokens is 160 because the 90th percentile of the input ids length is 160"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
